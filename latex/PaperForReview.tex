% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{longtable}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Re-implementation of Monocular Depth Estimation}

\author{Xiaofeng Lin, Saketh Thota, Abhinav Alluri, Lauren Stanciel, Jake Lane\\
University of Michigan\\
Ann Arbor, MI, USA\\
{\tt\small \{linxiaof,saketht,alluriab,jakelane,stanciel\}@umich.edu}
}
\maketitle

%%%%%%%%% ABSTRACT
% \begin{abstract}
%    The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
%    Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
%    The abstract is to be in 10-point, single-spaced type.
%    Leave two blank lines after the Abstract, then begin the main text.
%    Look at previous CVPR abstracts to get a feel for style and length.
% \end{abstract}

% - START OF INTRO -
\section{Introduction}
\label{sec:intro}

Depth estimation is a common computer vision building block, and refers to the
process by which the distance of an object from a reference point is determined.
This reference point is a camera in our case as our visual data was collected
using video cameras and is represented as RGB images. With monocular or
single-view depth estimation, we only have one image at the time of testing,
which makes this problem a geometrically impossible version of the commonly
known correspondence problem which performs inference with multiple images.
Using convolutional neural networks (CNNs), however, is one way of fairly
accurately estimating distance given a single test image.

Producing a dense depth map of a 3D space is very useful in a variety of
applications. This includes scene understanding for navigation, image refocusing
for portrait images, and even augmented reality. Furthermore, depth estimation
is crucial in many areas of robotics, particularly robotic-assisted surgery. A
lack of 3D spatial perception can hinder the surgical skills or even limit
awareness during a medical procedure and depth estimation can provide this
valuable information needed to adjust and improve such robotic-assisted surgery.

As highlighted in Alhashim et. al, there are often large perturbations in the
depth estimations by even the most state-of-the-art CNNs we have currently.
In particular, the deep ordinal regression network (DORN) from H. Fu et. al was
referenced and compared to the work by Alhashim et. al. In their findings, their
simpler encoder-decoder architecture was able to produce estimations of higher
accuracy and quality than those generated by DORN.

In order to better understand how and why this approach to depth estimation
works, we decided to reimplement this encoder-decoder architecture and attempt
to improve upon its architecture and parameters to minimize loss and produce
high quality dense depth maps for images in the NYUv2 dataset. Finally, we will
use various quantitative and qualitative metrics to assess our success in doing
so.

% - END OF INTRO -

% - START OF APPR -
\section{Approach}
\label{sec:appr}

\subsection{Model Architecture}

We started our model by using the encoder-decoder architecture from
Alhashim et. al. In its encoder, it uses a transfer learning approach with image
encoders that were originally designed for image classification. This encoder
is the DenseNet-169 network pretrained on ImageNet with the last layer removed.
Instead of applying this last layer, we input this vector into a series of
upsampling layers at only half the input resolution.

In our model, we added a small Unet model on top of the original model. It
consisted of 1 convolutional block with downsampling after each layer,
1 convolutional layer at the bottom of the downsampling, and then 1 more
convolutional block with upsampling. The exact details of our network can be
seen in Table 1 below.

\begin{table}[h]
  \footnotesize
  \centering
  \begin{tabular}{c|c|c|p{2.25cm}}
                             & Layer           & Output                      & Function                                      \\
    \hline
                             & Input           & $480 \times 640 \times 3$   &                                               \\
    \hline
    \multirow{4}{*}{Encoder}
                             & Conv$_1$        & $240 \times 320 \times 64$  & DenseNet Conv$_1$                             \\
                             & Pool$_1$        & $120 \times 160 \times 64$  & DenseNet Pool$_1$                             \\
                             & Pool$_2$        & $60 \times 80 \times 128$   & DenseNet Pool$_2$                             \\
                             & Pool$_3$        & $30 \times 40 \times 256$   & DenseNet Pool$_3$                             \\
    \hline
    \multirow{4}{*}{Decoder} & Conv$_2$        & $15 \times 20 \times 1664$  & Convolution $1\times 1$ of DenseNet Block$_4$ \\
                             & Up$_1$          & $30 \times 40 \times 1664$  & Upsample $2\times 2$                          \\
                             & Concat$_1$      & $30 \times 40 \times 832$   & Concatenate Pool$_3$                          \\
                             & Up$_1$-Conv$_A$ & $30 \times 40 \times 832$   & Convolution $3\times 3$                       \\
                             & Up$_1$-Conv$_B$ & $30 \times 40 \times 832$   & Convolution $3\times 3$                       \\
                             & Up$_2$          & $60 \times 80 \times 832$   & Upsample $2\times 2$                          \\
                             & Concat$_2$      & $60 \times 80 \times 960$   & Concatenate Pool$_2$                          \\
                             & Up$_2$-Conv$_A$ & $60 \times 80 \times 416$   & Convolution $3\times 3$                       \\
                             & Up$_2$-Conv$_B$ & $60 \times 80 \times 416$   & Convolution $3\times 3$                       \\
                             & Up$_3$          & $120 \times 160 \times 416$ & Upsample $2\times 2$                          \\
                             & Concat$_3$      & $120 \times 160 \times 480$ & Concatenate Pool$_1$                          \\
                             & Up$_3$-Conv$_A$ & $120 \times 160 \times 208$ & Convolution $3\times 3$                       \\
                             & Up$_3$-Conv$_B$ & $120 \times 160 \times 208$ & Convolution $3\times 3$                       \\
                             & Up$_4$          & $240 \times 320 \times 208$ & Upsample $2\times 2$                          \\
                             & Concat$_3$      & $240 \times 320 \times 272$ & Concatenate Conv$_1$                          \\
                             & Up$_2$-Conv$_A$ & $240 \times 320 \times 104$ & Convolution $3\times 3$                       \\
                             & Up$_2$-Conv$_B$ & $240 \times 320 \times 104$ & Convolution $3\times 3$                       \\
                             & Conv$_3$        & $240 \times 320 \times 1$   & Convolution $3\times 3$                       \\
    \hline
    \multirow{2}{*}{Unet}    & X                                                                                             \\
                             & X                                                                                             \\
    \hline
  \end{tabular}
\end{table}

\subsection{Hyperparameters and Loss}

%%%%%%%%% REFERENCES
{\small
  \bibliographystyle{ieee_fullname}
  \bibliography{egbib}
}

\end{document}
